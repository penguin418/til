{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "입력된 문자열을 연산가능한 의미있는 단위(token)으로 나누는 과정\n",
        "\n",
        "문장은 단어와 단어가 배치된 순서의 영향을 많이 받으므로 머신 러닝에선 문장을 숫자로 바꿔야 함.\n",
        "\n"
      ],
      "metadata": {
        "id": "v8Kk8PBFFOYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as  tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'I am a student.',\n",
        "    'I am a teacher.'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('word_index', word_index)\n",
        "\n",
        "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "print('tokenized_sentences', tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxf_QXw0HZv8",
        "outputId": "86499a67-8506-4e7b-fd26-90b790393037"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4], [1, 2, 3, 5]]\n",
            "{'i': 1, 'am': 2, 'a': 3, 'student': 4, 'teacher': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어는 조사와 어미가 있는 교착어라서 띄어쓰기만으로는 토큰화가 어려움.\n",
        "\n",
        "의미를 가진 최소 단위인 형태소 기반으로 분석해야 함.\n",
        "\n",
        "konlpy 에서 제공하는 Open Korean Text 로 형태소 분석"
      ],
      "metadata": {
        "id": "RIBUG0bIcsHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install konlpy\n",
        "sentences = [\n",
        "    \"내가 여섯 살 때, 원시림을 다룬 '자연의 이야기'라는 책에서 굉장한 그림 하나를 본 적이 있다.\",\n",
        "    \"그건 맹수를 한입페 삼키는 보아뱁이었는데, 옮겨 그리면 아래 그림과 같다.\",\n",
        "    \"그 책에는 이렇게 쓰여 있었다.\",\n",
        "    \"'보아뱀은 씹지도 않고 산 채로 먹이를 삼키고, 그런 다음엔 소화를 위해 여섯 달 동안 꿈쩍도 하지 않고 잠만 잔다'고.\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHR_yz1EeKcJ",
        "outputId": "3b19b539-4145-49b7-f2f4-a685898e9cc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "import tensorflow as  tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "okt = Okt()\n",
        "dictionary = []\n",
        "for sentence in sentences:\n",
        "  dictionary.extend(okt.morphs(sentence))\n",
        "print(dictionary)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(dictionary)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('word_index', word_index)\n",
        "\n",
        "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "print('tokenized_sentences', tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2qCmd3Sdj1-",
        "outputId": "b7a5b5d0-0a0b-465a-837f-e1d7d709e4f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['내', '가', '여섯', '살', '때', ',', '원시림', '을', '다룬', \"'\", '자연', '의', '이야기', \"'\", '라는', '책', '에서', '굉장한', '그림', '하나', '를', '본', '적', '이', '있다', '.', '그건', '맹수', '를', '한입페', '삼키는', '보아', '뱁이었는데', ',', '옮겨', '그리면', '아래', '그림', '과', '같다', '.', '그', '책', '에는', '이렇게', '쓰여', '있었다', '.', \"'\", '보아뱀', '은', '씹지도', '않고', '산', '채', '로', '먹이', '를', '삼키고', ',', '그런', '다음', '엔', '소화', '를', '위해', '여섯', '달', '동안', '꿈쩍', '도', '하지', '않고', '잠', '만', '잔다', \"'\", '고', '.']\n",
            "word_index {\"'\": 1, '를': 2, '여섯': 3, '책': 4, '그림': 5, '않고': 6, '내': 7, '가': 8, '살': 9, '때': 10, '원시림': 11, '을': 12, '다룬': 13, '자연': 14, '의': 15, '이야기': 16, '라는': 17, '에서': 18, '굉장한': 19, '하나': 20, '본': 21, '적': 22, '이': 23, '있다': 24, '그건': 25, '맹수': 26, '한입페': 27, '삼키는': 28, '보아': 29, '뱁이었는데': 30, '옮겨': 31, '그리면': 32, '아래': 33, '과': 34, '같다': 35, '그': 36, '에는': 37, '이렇게': 38, '쓰여': 39, '있었다': 40, '보아뱀': 41, '은': 42, '씹지도': 43, '산': 44, '채': 45, '로': 46, '먹이': 47, '삼키고': 48, '그런': 49, '다음': 50, '엔': 51, '소화': 52, '위해': 53, '달': 54, '동안': 55, '꿈쩍': 56, '도': 57, '하지': 58, '잠': 59, '만': 60, '잔다': 61, '고': 62}\n",
            "tokenized_sentences [[3, 9, 10, 13, 19, 5, 21, 24], [25, 27, 28, 31, 32, 33, 35], [36, 38, 39, 40], [43, 6, 44, 48, 49, 53, 3, 54, 55, 58, 6]]\n"
          ]
        }
      ]
    }
  ]
}
